{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lda import *\n",
    "from gensim.utils import simple_preprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_SAVED = False\n",
    "PRINT_COHERENCE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "We will create bigrams and train LDA models for each of the four datasets we have. We will use the validation dataset to test the impact of using LDA output as features for the model and the test dataset to evaluate cross-domain generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit_df_train = pd.read_csv(\"./../../data/implicithate_train.csv\")\n",
    "lda_implicit, vectors_implicit_train, coherence_implicit = lda(implicit_df_train, load_from=\"../../data/lda_implicit.model\") if LOAD_SAVED else lda(implicit_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.3919226698945722\n"
     ]
    }
   ],
   "source": [
    "if PRINT_COHERENCE:\n",
    "    print('Coherence Score:', coherence_implicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_SAVED:\n",
    "    lda_implicit.save('../../saved-models/lda_implicit.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVID Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df_train = pd.read_csv(\"./../../data/covidhate_train.csv\")\n",
    "lda_covid, vectors_covid_train, coherence_covid = lda(covid_df_train, load_from=\"../../saved-models/lda_covid.model\") if LOAD_SAVED else lda(covid_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.4378289569222037\n"
     ]
    }
   ],
   "source": [
    "if PRINT_COHERENCE:\n",
    "    print('Coherence Score:', coherence_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_SAVED:\n",
    "    lda_covid.save('../../saved-models/lda_covid.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offensval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "offenseval_df_train = pd.read_csv(\"./../../data/offenseval_train.csv\")\n",
    "lda_offenseval, vectors_offenseval_train, coherence_offenseval = lda(offenseval_df_train, load_from=\"../../saved-models/lda_offenseval.model\") if LOAD_SAVED else lda(offenseval_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.33806384735719547\n"
     ]
    }
   ],
   "source": [
    "if PRINT_COHERENCE:\n",
    "    print('Coherence Score:', coherence_offenseval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_SAVED:\n",
    "    lda_offenseval.save('../../saved-models/lda_offenseval.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SVMs with LDA features as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create/Validate the SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implicit Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(vectors_implicit_train)\n",
    "y = np.array(implicit_df_train['label'])\n",
    "svm_implicit = LinearSVC(tol=1e-3, class_weight=\"balanced\").fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit_df_val = pd.read_csv(\"./../../data/implicithate_val.csv\")\n",
    "implicit_df_val = create_tokenized_column(implicit_df_val)\n",
    "implicit_bigrams_val = create_bigrams(implicit_df_val[\"tokenized\"])\n",
    "implicit_dict_val, implicit_corpus_val = get_dict_and_corpus(implicit_bigrams_val)\n",
    "implicit_val_vectors = get_vectors(lda_implicit, implicit_corpus_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    non-hate       0.62      0.62      0.62      2150\n",
      "        hate       0.36      0.37      0.36      1284\n",
      "\n",
      "    accuracy                           0.52      3434\n",
      "   macro avg       0.49      0.49      0.49      3434\n",
      "weighted avg       0.52      0.52      0.52      3434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = svm_implicit.predict(implicit_val_vectors)\n",
    "print(classification_report(implicit_df_val['label'], preds, labels=[0, 1], target_names=['non-hate', 'hate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COVID Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(vectors_covid_train)\n",
    "y = np.array(covid_df_train['label'])\n",
    "svm_covid = LinearSVC(tol=1e-3, class_weight=\"balanced\").fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df_val = pd.read_csv(\"./../../data/covidhate_val.csv\")\n",
    "covid_df_val = create_tokenized_column(covid_df_val)\n",
    "covid_bigrams_val = create_bigrams(covid_df_val[\"tokenized\"])\n",
    "covid_dict_val, covid_corpus_val = get_dict_and_corpus(covid_bigrams_val)\n",
    "covid_val_vectors = get_vectors(lda_covid, covid_corpus_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    non-hate       0.86      0.58      0.69       312\n",
      "        hate       0.16      0.45      0.24        55\n",
      "\n",
      "    accuracy                           0.56       367\n",
      "   macro avg       0.51      0.52      0.46       367\n",
      "weighted avg       0.75      0.56      0.62       367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = svm_covid.predict(covid_val_vectors)\n",
    "print(classification_report(covid_df_val['label'], preds, labels=[0, 1], target_names=['non-hate', 'hate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offenseval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(vectors_offenseval_train)\n",
    "y = np.array(offenseval_df_train['label'])\n",
    "svm_offenseval = LinearSVC(tol=1e-3, class_weight=\"balanced\").fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "offenseval_df_val = pd.read_csv(\"./../../data/offenseval_val.csv\")\n",
    "offenseval_df_val = create_tokenized_column(offenseval_df_val)\n",
    "offenseval_bigrams_val = create_bigrams(offenseval_df_val[\"tokenized\"])\n",
    "offenseval_dict_val, offenseval_corpus_val = get_dict_and_corpus(offenseval_bigrams_val)\n",
    "offenseval_val_vectors = get_vectors(lda_offenseval, offenseval_corpus_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    non-hate       0.67      0.47      0.55      1733\n",
      "        hate       0.36      0.57      0.44       915\n",
      "\n",
      "    accuracy                           0.50      2648\n",
      "   macro avg       0.52      0.52      0.50      2648\n",
      "weighted avg       0.56      0.50      0.51      2648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = svm_offenseval.predict(offenseval_val_vectors)\n",
    "print(classification_report(offenseval_df_val['label'], preds, labels=[0, 1], target_names=['non-hate', 'hate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the SVMs using TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"from\", \"subject\", \"re\", \"edu\", \"use\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implicit Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit_tfidf = TfidfVectorizer(stop_words=stop_words, max_features=5000)\n",
    "implicit_tfidf.fit(implicit_df_train['text'])\n",
    "implicit_tfidf_train = implicit_tfidf.transform(implicit_df_train['text'])\n",
    "implicit_tfidf_val = implicit_tfidf.transform(implicit_df_val['text'])\n",
    "svm_implicit = LinearSVC(tol=1e-3, class_weight=\"balanced\").fit(implicit_tfidf_train, implicit_df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    non-hate       0.77      0.73      0.75      2150\n",
      "        hate       0.58      0.64      0.61      1284\n",
      "\n",
      "    accuracy                           0.70      3434\n",
      "   macro avg       0.68      0.68      0.68      3434\n",
      "weighted avg       0.70      0.70      0.70      3434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = svm_implicit.predict(implicit_tfidf_val)\n",
    "print(classification_report(implicit_df_val['label'], preds, labels=[0, 1], target_names=['non-hate', 'hate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COVID Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_tfidf = TfidfVectorizer(stop_words=stop_words, max_features=5000)\n",
    "covid_tfidf.fit(covid_df_train['text'])\n",
    "covid_tfidf_train = covid_tfidf.transform(covid_df_train['text'])\n",
    "covid_tfidf_val = covid_tfidf.transform(covid_df_val['text'])\n",
    "svm_covid = LinearSVC(tol=1e-3, class_weight=\"balanced\").fit(covid_tfidf_train, covid_df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    non-hate       0.93      0.93      0.93       312\n",
      "        hate       0.60      0.62      0.61        55\n",
      "\n",
      "    accuracy                           0.88       367\n",
      "   macro avg       0.76      0.77      0.77       367\n",
      "weighted avg       0.88      0.88      0.88       367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = svm_covid.predict(covid_tfidf_val)\n",
    "print(classification_report(covid_df_val['label'], preds, labels=[0, 1], target_names=['non-hate', 'hate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offenseval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "offenseval_tfidf = TfidfVectorizer(stop_words=stop_words, max_features=5000)\n",
    "offenseval_tfidf.fit(offenseval_df_train['text'])\n",
    "offenseval_tfidf_train = offenseval_tfidf.transform(offenseval_df_train['text'])\n",
    "offenseval_tfidf_val = offenseval_tfidf.transform(offenseval_df_val['text'])\n",
    "svm_offenseval = LinearSVC(tol=1e-3, class_weight=\"balanced\").fit(offenseval_tfidf_train, offenseval_df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    non-hate       0.79      0.77      0.78      1733\n",
      "        hate       0.59      0.62      0.61       915\n",
      "\n",
      "    accuracy                           0.72      2648\n",
      "   macro avg       0.69      0.70      0.69      2648\n",
      "weighted avg       0.72      0.72      0.72      2648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = svm_offenseval.predict(offenseval_tfidf_val)\n",
    "print(classification_report(offenseval_df_val['label'], preds, labels=[0, 1], target_names=['non-hate', 'hate']))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ec2030db7bfae298aff43999c7d9f1264b161236d59f5acd9bdfb7ddab7d446"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cs6471')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
