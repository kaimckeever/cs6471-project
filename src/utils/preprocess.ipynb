{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ee29ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436a4f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add root path to system path:  D:\\Projets\\Georgia Tech\\Comp Social Science\\cs6471-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.dirname(os.path.dirname(os.path.abspath(os.path.join('.'))))\n",
    "if module_path not in sys.path:\n",
    "    print('Add root path to system path: ', module_path)\n",
    "    sys.path.append(module_path)\n",
    "module_path += '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097bf1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Richard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from preprocess_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc5d202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'BasicLSTM'\n",
    "fix_length = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccc43f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file loaded and formatted..\n",
      "data split into train/val/test\n",
      "field objects created\n",
      "fields and dataset object created\n",
      "vocabulary built..\n",
      "train_data <generator object Dataset.__getattr__ at 0x00000215389180B0>\n",
      "val_data <generator object Dataset.__getattr__ at 0x00000215389180B0>\n",
      "test_data <generator object Dataset.__getattr__ at 0x00000215389180B0>\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocessing Offenseval datasets\n",
    "train_dataset_name = 'offenseval'\n",
    "test_dataset_name = 'offenseval'\n",
    "field, tokenizer, train_data, val_data, test_data = get_datasets(train_dataset_name, test_dataset_name, model_type, fix_length, module_path=module_path)\n",
    "print('train_data', len(train_data))\n",
    "print('val_data', len(val_data))\n",
    "print('test_data', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "672e9539",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mD:\\Projets\\Georgia Tech\\Comp Social Science\\cs6471-project\\src\\utils\\preprocess_utils.py\u001b[0m in \u001b[0;36mget_datasets\u001b[1;34m(train_dataset_name, test_dataset_name, model_type, fix_length, module_path)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfix_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;31m# preprocessing of the train/validation tweets, then test tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m     \u001b[0mtweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_training_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m     \u001b[0mtweets_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_test_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_dataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"file loaded and formatted..\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Projets\\Georgia Tech\\Comp Social Science\\cs6471-project\\src\\utils\\preprocess_utils.py\u001b[0m in \u001b[0;36mformat_training_file\u001b[1;34m(dataset_name, module_path)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'https.*[^ ]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'URL'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'http.*[^ ]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'URL'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memoji\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdemojize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'(:.*?:)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr' \\1 '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' +'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-gpu-nlp\\lib\\site-packages\\emoji\\core.py\u001b[0m in \u001b[0;36mdemojize\u001b[1;34m(string, use_aliases, delimiters, language, version, handle_version)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdelimiters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdelimiters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mget_emoji_regexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'\\ufe0e'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'\\ufe0f'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocessing SBF datasets\n",
    "train_dataset_name = 'SBF'\n",
    "test_dataset_name = 'SBF'\n",
    "field, tokenizer, train_data, val_data, test_data = get_datasets(train_dataset_name, test_dataset_name, model_type, fix_length, module_path=module_path)\n",
    "print('train_data', len(train_data))\n",
    "print('val_data', len(val_data))\n",
    "print('test_data', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03d3d5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file loaded and formatted..\n",
      "data split into train/val/test\n",
      "field objects created\n",
      "fields and dataset object created\n",
      "vocabulary built..\n",
      "train_data 13734\n",
      "val_data 3434\n",
      "test_data 4291\n",
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocessing Implicithate datasets\n",
    "train_dataset_name = 'implicithate'\n",
    "test_dataset_name = 'implicithate'\n",
    "field, tokenizer, train_data, val_data, test_data = get_datasets(train_dataset_name, test_dataset_name, model_type, fix_length, module_path=module_path)\n",
    "print('train_data', len(train_data))\n",
    "print('val_data', len(val_data))\n",
    "print('test_data', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f68a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372be8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba080bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f1396b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
